{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment-Analysis.ipynb","provenance":[],"collapsed_sections":["d_R9Ev-0srpc","PlwQoIqNsy-1","CQyFCxSJs3jJ","xVRVOGEEthVD","7RFvXIlduElx","nldnmdGxuP5p","LcjT5zvFuetX","vgwyDOmBwC3v","u8TpabTpwPgu","kyubqqadxHiM","yiSraIqRxtam","uwLfMOkcx2bP","iXE-OKD85aGN","Tv3IkjTV5fwM","BZ4a9vVY5lcc"],"authorship_tag":"ABX9TyMo68JHmzcvGfZO8oW7lFfo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Predicting E-Commerce Product Recommendations from Reviews \n","\n","\n","![](https://github.com/dipanjanS/feature_engineering_session_dhs18/blob/master/ecommerce_product_ratings_prediction/clothing_banner.jpg?raw=1)\n","\n","This is a classic NLP problem dealing with data from an e-commerce store focusing on women's clothing. Each record in the dataset is a customer review which consists of the review title, text description and a recommendation 0 or 1) for a product amongst other features\n","\n","\n","__Main Objective:__ Leverage the review text attributes and other features if needed to predict the recommendation (classification)\n","\n","---\n","---\n","\n","\n","- Experiment 1: Basic NLP Count based Features & Age, Feedback Count\n","  - Training a Logistic Regression Model\n","  - Model Evaluation Metrics - Quick Refresher\n","- Experiment 2: Features from Sentiment Analysis\n","  - Leveraging Text Sentiment\n","  - Model Training and Evaluation\n","- Text Pre-processing and Wrangling\n","- Experiment 3: Modeling based on Bag of Words based Features - 1-grams\n","  - Use the following default config for count vectorizer\n","  - Model Training and Evaluation\n","- Experiment 4: Modeling with Bag of Words based Features - 2-grams\n","  - Model Training and Evaluation\n","- Experiment 5: Adding Bag of Words based Features - 3-grams\n","  - Model Training and Evaluation\n","- Experiment 6: Adding Bag of Words based Features - 3-grams with Feature Selection\n","- Experiment 7: Combining Bag of Words based Features - 3-grams with Feature Selection and the Structured Features\n","  - Coverting dense features into sparse format\n","  - Combine the features using `hstack`\n","  - Model Training and Evaluation\n","- Experiment 8: Modeling on FastText Averaged Document Embeddings\n","  - Generate document level embeddings\n","  "],"metadata":{"id":"3JjCafFxsdla"}},{"cell_type":"markdown","source":["# Load up basic dependencies"],"metadata":{"id":"d_R9Ev-0srpc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNg1h5NYmmsn"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import classification_report, confusion_matrix\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import nltk\n","import matplotlib.pyplot as plt\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","source":["!pip install contractions\n","!pip install textsearch\n","!pip install tqdm"],"metadata":{"id":"mC8sodK7svIi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load and View the Dataset\n","\n","The data is available at https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews from where you can download it.\n","\n","You can also access it from my [__GitHub Repo__](https://github.com/dipanjanS/text-analytics-with-python/blob/master/media) if needed.\n","\n","Following code enables it to get it easily from the web."],"metadata":{"id":"PlwQoIqNsy-1"}},{"cell_type":"code","source":["df = pd.read_csv('https://github.com/dipanjanS/text-analytics-with-python/raw/master/media/Womens%20Clothing%20E-Commerce%20Reviews%20-%20NLP.csv', keep_default_na=False)\n","df.head()"],"metadata":{"id":"RfNGiNCvsvvE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Basic Data Processing\n","\n","- Merge all review text attributes (title, text description) into one attribute\n","- Subset out columns of interest"],"metadata":{"id":"CQyFCxSJs3jJ"}},{"cell_type":"code","source":["df['Review'] = (df['Title'].map(str) +' '+ df['Review Text']).apply(lambda row: row.strip())\n","df['Recommended'] = df['Recommended IND']\n","df = df[['Review', 'Age', 'Positive Feedback Count', 'Recommended']]\n","df.head()\n","\n","# # Day 4\n","# df['Review'] = (df['Title'].map(str) +' '+ df['Review Text']).apply(lambda row: row.strip())\n","# df['Recommended'] = df['Recommended IND']\n","# df = df[['Review', 'Recommended']]\n","# df.head()"],"metadata":{"id":"Vl5YtT3Ts35h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Remove all records with no review text"],"metadata":{"id":"G302vwQCtRMb"}},{"cell_type":"code","source":["df = df[df['Review'] != '']\n","df.info()"],"metadata":{"id":"SXZMDPvUs4XV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## There is some imbalance in the data based on product recommendations"],"metadata":{"id":"6npDRlGxtcm_"}},{"cell_type":"code","source":["df['Recommended'].value_counts()"],"metadata":{"id":"uqC-qCR8s4Ud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Build train and test datasets"],"metadata":{"id":"xVRVOGEEthVD"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Recommended']), df['Recommended'], test_size=0.3, random_state=42)\n","X_train.shape, X_test.shape"],"metadata":{"id":"moJuIYIYs4R0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","Counter(y_train), Counter(y_test)"],"metadata":{"id":"zVYeYCeEs4O2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.head(3)"],"metadata":{"id":"aCOC9Dvttwt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train[:3]"],"metadata":{"id":"8NxirEmGtwrc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks like this should help us get features which can distinguish between good and bad products. Let's try it out on our dataset!"],"metadata":{"id":"_fIfePlnvbqL"}},{"cell_type":"markdown","source":["# Experiment 1: Basic NLP Count based Features & Age, Feedback Count\n","\n","A number of basic text based features can also be created which sometimes are helpful for improving text classification models. \n","Some examples are:\n","\n","- __Word Count:__ total number of words in the documents\n","- __Character Count:__ total number of characters in the documents\n","- __Average Word Density:__ average length of the words used in the documents\n","- __Puncutation Count:__ total number of punctuation marks in the documents\n","- __Upper Case Count:__ total number of upper count words in the documents\n","- __Title Word Count:__ total number of proper case (title) words in the documents"],"metadata":{"id":"7RFvXIlduElx"}},{"cell_type":"code","source":["import string\n","\n","X_train['char_count'] = X_train['Review'].apply(len)\n","X_train['word_count'] = X_train['Review'].apply(lambda x: len(x.split()))\n","X_train['word_density'] = X_train['char_count'] / (X_train['word_count']+1)\n","X_train['punctuation_count'] = X_train['Review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n","X_train['title_word_count'] = X_train['Review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n","X_train['upper_case_word_count'] = X_train['Review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n","\n","\n","X_test['char_count'] = X_test['Review'].apply(len)\n","X_test['word_count'] = X_test['Review'].apply(lambda x: len(x.split()))\n","X_test['word_density'] = X_test['char_count'] / (X_test['word_count']+1)\n","X_test['punctuation_count'] = X_test['Review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n","X_test['title_word_count'] = X_test['Review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n","X_test['upper_case_word_count'] = X_test['Review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"],"metadata":{"id":"c6x7VOJLtwo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.head()"],"metadata":{"id":"FuOTV8cAtwmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training a Logistic Regression Model \n","\n","A logistic regression model is easy to train, interpret and works well on a wide variety of NLP problems"],"metadata":{"id":"nldnmdGxuP5p"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression(C=1, random_state=42, solver='lbfgs', max_iter=1e4)"],"metadata":{"id":"j73yV56stwjd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Evaluation Metrics - Quick Refresher\n","\n","Just accuracy is never enough in datasets with a rare class problem.\n","\n","- __Precision:__ The positive predictive power of a model. Out of all the predictions made by a model for a class, how many are actually correct\n","- __Recall:__ The coverage or hit-rate of a model. Out of all the test data samples belonging to a class, how many was the model able to predict (hit or cover) correctly.\n","- __F1-score:__ The harmonic mean of the precision and recall"],"metadata":{"id":"nq7B496SuX5y"}},{"cell_type":"markdown","source":["# Experiment 2: Features from Sentiment Analysis "],"metadata":{"id":"YrmvFGbsv1Jn"}},{"cell_type":"markdown","source":["## Leveraging Text Sentiment\n","\n","Reviews are pretty subjective, opinionated and people often express stong emotions, feelings. \n","This makes it a classic case where the text documents here are a good candidate for extracting sentiment as a feature.\n","\n","The general expectation is that highly rated and recommended products (__label 1__) should have a __positive__ sentiment and products which are not recommended (__label 0__) should have a __negative__ sentiment.\n","\n","TextBlob is an excellent open-source library for performing NLP tasks with ease, including sentiment analysis. It also an a sentiment lexicon (in the form of an XML file) which it leverages to give both polarity and subjectivity scores. \n","\n","- The polarity score is a float within the range [-1.0, 1.0]. \n","- The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. \n","\n","Perhaps this could be used for getting some new features? Let's look at some basic examples.\n","\n","Source: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"],"metadata":{"id":"LcjT5zvFuetX"}},{"cell_type":"code","source":["import textblob\n","\n","textblob.TextBlob('This is an AMAZING pair of Jeans!').sentiment"],"metadata":{"id":"uAMumZFHvX3_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textblob.TextBlob('I really hated this UGLY T-shirt!!').sentiment"],"metadata":{"id":"PaijZn0AvYYI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remember this is unsupervised, lexicon-based sentiment analysis where we don't have any pre-labeled data saying which review migth have a positive or negative sentiment. We use the lexicon to determine this."],"metadata":{"id":"YAT1XD5Tv7IO"}},{"cell_type":"code","source":["x_train_snt_obj = X_train['Review'].apply(lambda row: textblob.TextBlob(row).sentiment)\n","X_train['Polarity'] = [obj.polarity for obj in x_train_snt_obj.values]\n","X_train['Subjectivity'] = [obj.subjectivity for obj in x_train_snt_obj.values]\n","\n","x_test_snt_obj = X_test['Review'].apply(lambda row: textblob.TextBlob(row).sentiment)\n","X_test['Polarity'] = [obj.polarity for obj in x_test_snt_obj.values]\n","X_test['Subjectivity'] = [obj.subjectivity for obj in x_test_snt_obj.values]"],"metadata":{"id":"rOp4OCa2uXgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.head()"],"metadata":{"id":"W6crvWo2s4MD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training and Evaluation"],"metadata":{"id":"vgwyDOmBwC3v"}},{"cell_type":"code","source":["lr.fit(X_train.drop(['Review'], axis=1), y_train)\n","predictions = lr.predict(X_test.drop(['Review'], axis=1))\n","\n","print(confusion_matrix(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"GRMNPyObwBLz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will probably get a better model than Experiment 1\n","\n","Can we still improve on our model since the recall of bad reviews is still pretty low?"],"metadata":{"id":"9eJkyi9PwHO2"}},{"cell_type":"markdown","source":["# Text Pre-processing and Wrangling\n","\n","We want to extract some specific features based on standard NLP feature engineering models like the classic Bag of Words model.\n","For this we need to clean and pre-process our text data. We will build a simple text pre-processor here since the main intent is to look at feature engineering strategies.\n","\n","We will focus on:\n","- Text Lowercasing\n","- Removal of contractions\n","- Removing unnecessary characters, numbers and symbols\n","- Stopword removal\n","\n","Source: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"],"metadata":{"id":"u8TpabTpwPgu"}},{"cell_type":"code","source":["import contractions\n","\n","contractions.fix('I didn\\'t like this t-shirt')"],"metadata":{"id":"9ky0_a6KwBFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","import contractions\n","import re\n","import tqdm\n","\n","# remove some stopwords to capture negation in n-grams if possible\n","stopwords = nltk.corpus.stopwords.words('english')\n","stopwords.remove('no')\n","stopwords.remove('not')\n","stopwords.remove('but')\n","\n","\n","def normalize_document(doc):\n","    # fix contractions\n","    doc = contractions.fix(doc)\n","    # remove special characters and digits\n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, flags=re.I|re.A)\n","    # lower case\n","    doc = doc.lower()\n","    # strip whitespaces\n","    doc = doc.strip()\n","    # tokenize document\n","    tokens = nltk.word_tokenize(doc)\n","    #filter stopwords out of document\n","    filtered_tokens = [token for token in tokens if token not in stopwords]\n","    # re-create document from filtered tokens\n","    doc = ' '.join(filtered_tokens)\n","    return doc\n","\n","def normalize_corpus(docs):\n","    norm_docs = []\n","    for doc in tqdm.tqdm(docs):\n","        norm_doc = normalize_document(doc)\n","        norm_docs.append(norm_doc)\n","\n","    return norm_docs"],"metadata":{"id":"YyoKEyGowBDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train['Clean Review'] = normalize_corpus(X_train['Review'].values)\n","X_test['Clean Review'] = normalize_corpus(X_test['Review'].values)"],"metadata":{"id":"Vu9q17sDwBAZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Let's remove the review column now since we don't need it anymore and restructure our dataframes"],"metadata":{"id":"w1ch0kGmw9VQ"}},{"cell_type":"code","source":["X_train = X_train[['Clean Review', 'Age', 'Positive Feedback Count', 'Polarity', 'Subjectivity']]\n","X_test = X_test[['Clean Review', 'Age', 'Positive Feedback Count', 'Polarity', 'Subjectivity']]\n","\n","X_train.head()"],"metadata":{"id":"6YGHis2VwA5K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extracting out the structured features from previous experiments\n","\n","__We will extract out the structured columns \\ features so we can use them right at the end after doing a few experiments with bag of words__\n","\n","`X_train_struct` and `X_test_struct` should contain only 4 columns i.e.\n","\n","- Age\n","- Positive Feedback Count\n","- Polarity\n","- Subjectivity"],"metadata":{"id":"njQHC1e9xBhn"}},{"cell_type":"code","source":["X_train_struct = X_train.drop(['Clean Review'], axis=1).reset_index(drop=True)\n","X_test_struct = X_test.drop(['Clean Review'], axis=1).reset_index(drop=True)\n","\n","X_train_struct.head()"],"metadata":{"id":"eFjjrIe5xCkn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 3: Modeling based on Bag of Words based Features - 1-grams\n","\n","This is perhaps the most simple vector space representational model for unstructured text. A vector space model is simply a mathematical model to represent unstructured text (or any other data) as numeric vectors, such that each dimension of the vector is a specific feature\\attribute. \n","\n","The bag of words model represents each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0) or even weighted values. \n","\n","The model’s name is such because each document is represented literally as a ‘bag’ of its own words, disregarding word orders, sequences and grammar.\n","\n","Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41"],"metadata":{"id":"kyubqqadxHiM"}},{"cell_type":"code","source":["train_clean_text = X_train['Clean Review']\n","test_clean_text = X_test['Clean Review']"],"metadata":{"id":"Ram4oP6AxDbI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Use the following default config for count vectorizer\n","\n","- `min_df` as 0.0\n","- `max_df` as 1.0\n","- `ngram_range` as (1,1)"],"metadata":{"id":"OBzJETilxdkb"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","cv = CountVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1, 1))\n","\n","X_traincv = cv.fit_transform(train_clean_text)\n","X_testcv = cv.transform(test_clean_text)"],"metadata":{"id":"WuiP-HOWxDY3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_traincv"],"metadata":{"id":"UK0c4199xDV7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training and Evaluation"],"metadata":{"id":"mRaAGNWtxlhJ"}},{"cell_type":"code","source":["lr.fit(X_traincv, y_train)\n","predictions = lr.predict(X_testcv)\n","\n","print(confusion_matrix(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"P5ixtZvkxDLL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This should look promising and far better that the previous models if you did it correctly\n","\n","Can we still improve on our model? Let's look at n-grams!"],"metadata":{"id":"XzChOKf0xp_Z"}},{"cell_type":"markdown","source":["# Experiment 4: Modeling with Bag of Words based Features - 2-grams\n","\n","We use the same feature engineering technique here except we consider both 1 and 2-grams as our features. "],"metadata":{"id":"yiSraIqRxtam"}},{"cell_type":"code","source":["cv = CountVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1, 2))\n","\n","X_traincv = cv.fit_transform(train_clean_text)\n","X_testcv = cv.transform(test_clean_text)"],"metadata":{"id":"ZfMu4E3oxshg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_traincv"],"metadata":{"id":"HALohyDqxyDd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training and Evaluation"],"metadata":{"id":"uwLfMOkcx2bP"}},{"cell_type":"code","source":["lr.fit(X_traincv, y_train)\n","predictions = lr.predict(X_testcv)\n","\n","print(confusion_matrix(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"F3qW3mFIxyAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You should be able to see some minor improvements"],"metadata":{"id":"LOQwOB6Mx8O3"}},{"cell_type":"markdown","source":["# Experiment 5: Adding Bag of Words based Features - 3-grams \n","\n","We use the same feature engineering technique here except we consider 1, 2 and 3-grams as our features."],"metadata":{"id":"iXE-OKD85aGN"}},{"cell_type":"code","source":["cv = CountVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1, 3))\n","\n","X_traincv = cv.fit_transform(train_clean_text)\n","X_testcv = cv.transform(test_clean_text)"],"metadata":{"id":"o6CIhPkCxx-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_traincv"],"metadata":{"id":"X5X4zbHGxx8E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training and Evaluation"],"metadata":{"id":"Tv3IkjTV5fwM"}},{"cell_type":"code","source":["lr.fit(X_traincv, y_train)\n","predictions = lr.predict(X_testcv)\n","\n","print(confusion_matrix(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"KBRiNji7xx5L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 6: Adding Bag of Words based Features - 3-grams with Feature Selection\n","\n","Set `min_df` as 3 in CountVectorizer and keep other params same as the previous experiment and notice the drop in features.\n","\n","We drop all words \\ n-grams which occur less than 3 times in all documents.\n","\n","How will the model perform now?"],"metadata":{"id":"BZ4a9vVY5lcc"}},{"cell_type":"code","source":["cv = CountVectorizer(min_df=3, max_df=1., ngram_range=(1, 3))\n","\n","X_traincv = cv.fit_transform(train_clean_text)\n","X_testcv = cv.transform(test_clean_text)"],"metadata":{"id":"ugUq_Pcn5k2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_traincv"],"metadata":{"id":"p4ocSSdtxxd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr.fit(X_traincv, y_train)\n","predictions = lr.predict(X_testcv)\n","\n","print(confusion_matrix(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"BtNjLiSU5pr3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 7: Combining Bag of Words based Features - 3-grams with Feature Selection and the Structured Features\n","\n","Let's combine our sparse BOW feature matrices with our structured features from earlier.\n","\n","We do need to convert those structured features into sparse format so we can concatenate them to the BOW features!"],"metadata":{"id":"DIT9LNb15tjy"}},{"cell_type":"code","source":["X_train_struct.values"],"metadata":{"id":"J7uBIDFy5pos"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Coverting dense features into sparse format"],"metadata":{"id":"7jJORREO5yoJ"}},{"cell_type":"code","source":["from scipy import sparse"],"metadata":{"id":"HeLCTPtJ5xM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sparse.csr_matrix(X_train_struct)"],"metadata":{"id":"B3kG8QEw5xI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_traincv"],"metadata":{"id":"gHF2k-CT5xFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Combine the features using `hstack`\n","\n","Check documentation if needed, it should be straightforward"],"metadata":{"id":"6rBPdMOp55rY"}},{"cell_type":"code","source":["from scipy.sparse import hstack\n","\n","X_train_combined = hstack([sparse.csr_matrix(X_train_struct), \n","                           X_traincv])\n","X_test_combined = hstack([sparse.csr_matrix(X_test_struct), X_testcv])"],"metadata":{"id":"8pyrQ9RC5pQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_combined"],"metadata":{"id":"xEvjNNZ-58RP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training and Evaluation"],"metadata":{"id":"mYim3m7G_Muc"}},{"cell_type":"code","source":["lr.fit(X_train_combined, y_train)\n","predictions = lr.predict(X_test_combined)\n","\n","print(confusion_matrix(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"4LTCgHtI58PD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 8: Modeling on FastText Averaged Document Embeddings\n","\n","## Build the FastText embedding model here\n","\n","Remember more the iterations usually better the embeddings but the more time it will take depending on your system CPU\n","\n","10 iterations might take 5 mins"],"metadata":{"id":"qjVwdeaA_SkL"}},{"cell_type":"code","source":["%%time\n","\n","from gensim.models import FastText\n","\n","tokenized_docs_train = [doc.split() for doc in train_clean_text]\n","# sample config params size: 300, window: 30, min_count=2 or more, iter=10\n","ft_model = FastText(tokenized_docs_train, size=300, window=30, min_count=2, workers=4, sg=1, iter=10)"],"metadata":{"id":"zdCJe62Q58Lw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate document level embeddings\n","\n","Word embedding models give us an embedding for each word, how can we use it for downstream ML\\DL tasks? one way is to flatten it or use sequential models. A simpler approach is to average all word embeddings for words in a document and generate a fixed-length document level emebdding"],"metadata":{"id":"eiXV_1fV_YPx"}},{"cell_type":"code","source":["def averaged_word2vec_vectorizer(corpus, model, num_features):\n","    vocabulary = set(model.wv.index2word)\n","    \n","    def average_word_vectors(words, model, vocabulary, num_features):\n","        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n","        nwords = 0.\n","        \n","        for word in words:\n","            if word in vocabulary: \n","                nwords = nwords + 1.\n","                feature_vector = np.add(feature_vector, model.wv[word])\n","        if nwords:\n","            feature_vector = np.divide(feature_vector, nwords)\n","\n","        return feature_vector\n","\n","    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n","                    for tokenized_sentence in corpus]\n","    return np.array(features)"],"metadata":{"id":"WgiNLlqy58Jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_docs_train = [doc.split() for doc in train_clean_text]\n","tokenized_docs_test = [doc.split() for doc in test_clean_text]\n","\n","Xtrain_doc_vecs_ft = averaged_word2vec_vectorizer(tokenized_docs_train, ft_model, 300)\n","Xtest_doc_vecs_ft = averaged_word2vec_vectorizer(tokenized_docs_test, ft_model, 300)\n","\n","Xtrain_doc_vecs_ft.shape"],"metadata":{"id":"7zUjVfod58G4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training and Evaluation"],"metadata":{"id":"7SiYV2iZABgx"}},{"cell_type":"code","source":["lr.fit(Xtrain_doc_vecs_ft, y_train)\n","predictions = lr.predict(Xtest_doc_vecs_ft)\n","\n","print(confusion_matrix(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"PPDPRyMV58EN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 9: Combine FastText Vectors + Structured Features and build a model"],"metadata":{"id":"r2sezbzIAFa5"}},{"cell_type":"code","source":["X_train_combined = np.concatenate((X_train_struct, Xtrain_doc_vecs_ft),axis=1)\n","X_test_combined = np.concatenate((X_test_struct, Xtest_doc_vecs_ft),axis=1)"],"metadata":{"id":"CNAo_qyZAFug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training and Evaluation"],"metadata":{"id":"taMvy4EeAJDF"}},{"cell_type":"code","source":["lr.fit(X_train_combined, y_train)\n","predictions = lr.predict(X_test_combined)\n","\n","print(confusion_matrix(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"CUXGFC_wAGRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 10: Train Classfier with CNN + FastText Embeddings & Evaluate Performance on Test Data\n","\n","__Note:__ Skip FastText Embeddings part if it takes too much time to download or load it since it does consume a good amount of memory to load the pretrained embeddings.\n","\n","If you want to load pre-trained embeddings use a slightly smaller file than the one we used in live-coding which had over 2 million words. Here is the link to get embeddings from facebook's pre-trained fasttext model.\n","\n","https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n","\n","__Hint:__ Use the code from the live-coding session to download and load relevant embeddings from the above dataset"],"metadata":{"id":"h6qmWGXZAZI5"}},{"cell_type":"code","source":["train_clean_text = X_train['Clean Review']\n","test_clean_text = X_test['Clean Review']"],"metadata":{"id":"cosqsYIXAGOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n","# fit the tokenizer on the documents\n","t.fit_on_texts(train_clean_text)\n","t.word_index['<PAD>'] = 0"],"metadata":{"id":"LNM11kqmAGL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), \n","      min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), \n","      t.word_index['<UNK>'])"],"metadata":{"id":"ZW7BQMnlAGJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sequences = t.texts_to_sequences(train_clean_text)\n","test_sequences = t.texts_to_sequences(test_clean_text)"],"metadata":{"id":"SzctYHp4AGHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Vocabulary size={}\".format(len(t.word_index)))\n","print(\"Number of Documents={}\".format(t.document_count))"],"metadata":{"id":"yPYAdWLMAGBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max(len(i) for i in train_sequences)"],"metadata":{"id":"dAooQApZAiMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max(len(doc.split()) for doc in train_clean_text)"],"metadata":{"id":"zvZb0INKAiJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist([len(item) for item in train_sequences], bins=30);"],"metadata":{"id":"nKGr8KH_AiGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_SEQUENCE_LENGTH = 121\n","\n","# pad dataset to a maximum review length in words\n","train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","test_seqs = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","train_seqs.shape, test_seqs.shape"],"metadata":{"id":"KPfbjwVDAiDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VOCAB_SIZE = len(t.word_index)\n","EMBED_SIZE = 300\n","EPOCHS=100\n","BATCH_SIZE=32"],"metadata":{"id":"EqnL0f35AiBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"],"metadata":{"id":"a2ogVv2LAh-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip wiki-news-300d-1M.vec.zip"],"metadata":{"id":"lWpnAMVHAskV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word2idx = t.word_index\n","FASTTEXT_INIT_EMBEDDINGS_FILE = './wiki-news-300d-1M.vec'\n","\n","\n","def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):    \n","    \n","    def get_coefs(word,*arr): \n","        return word, np.asarray(arr, dtype='float32')\n","    \n","    embeddings_index = dict(get_coefs(*row.split(\" \")) \n","                                for row in open(embedding_file_path, encoding=\"utf8\", errors='ignore') \n","                                    if len(row)>100)\n","\n","    all_embs = np.stack(embeddings_index.values())\n","    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n","    embed_size = all_embs.shape[1]\n","\n","    nb_words = min(max_features, len(word_to_index))\n","    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n","    \n","    for word, idx in word_to_index.items():\n","        if idx >= max_features: \n","            continue\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None: \n","            embedding_matrix[idx] = embedding_vector\n","\n","    return embedding_matrix"],"metadata":{"id":"vvO1c4xFAshl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, \n","                                           max_features=VOCAB_SIZE, \n","                                           embedding_size=EMBED_SIZE, \n","                                           embedding_file_path=FASTTEXT_INIT_EMBEDDINGS_FILE)\n","ft_embeddings.shape"],"metadata":{"id":"tFDqUq_aAse2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the model\n","model = tf.keras.models.Sequential()\n","\n","model.add(tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE,\n","                                    weights=[ft_embeddings],\n","                                    trainable=True,\n","                                    input_length=MAX_SEQUENCE_LENGTH))\n","\n","model.add(tf.keras.layers.Conv1D(filters=256, kernel_size=4, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n","\n","model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n","\n","model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n","\n","model.add(tf.keras.layers.Flatten())\n","\n","model.add(tf.keras.layers.Dense(256, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.25))\n","model.add(tf.keras.layers.Dense(256, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.25))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"kpdKc8i8AscV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the model\n","es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                      patience=2,\n","                                      restore_best_weights=True,\n","                                      verbose=1)\n","\n","model.fit(train_seqs, y_train, \n","          validation_split=0.1,\n","          epochs=EPOCHS, \n","          batch_size=BATCH_SIZE, \n","          shuffle=True,\n","          callbacks=[es],\n","          verbose=1)"],"metadata":{"id":"Y6DYvoqSAsZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model.predict_classes(test_seqs).ravel()\n","\n","print(classification_report(y_test, predictions))\n","pd.DataFrame(confusion_matrix(y_test, predictions))"],"metadata":{"id":"5NmYVtw6AsVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 11: Train Classfier with LSTM + FastText Embeddings & Evaluate Performance on Test Data\n","\n","__Note:__ Skip FastText Embeddings part if it takes too much time to download or load it since it does consume a good amount of memory to load the pretrained embeddings."],"metadata":{"id":"P9dSSy2nA3_L"}},{"cell_type":"code","source":["LSTM_DIM = 256\n","\n","# create the model\n","model = tf.keras.models.Sequential()\n","\n","model.add(tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE,\n","                                    weights=[ft_embeddings],\n","                                    trainable=True,\n","                                    input_length=MAX_SEQUENCE_LENGTH))\n","\n","#model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=True))\n","model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False))\n","\n","model.add(tf.keras.layers.Flatten())\n","\n","model.add(tf.keras.layers.Dense(256, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.25))\n","model.add(tf.keras.layers.Dense(256, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.25))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"qppOqiOeAsSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the model\n","es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                      patience=2,\n","                                      restore_best_weights=True,\n","                                      verbose=1)\n","\n","model.fit(train_seqs, y_train, \n","          validation_split=0.1,\n","          epochs=EPOCHS, \n","          batch_size=BATCH_SIZE, \n","          shuffle=True,\n","          callbacks=[es],\n","          verbose=1)"],"metadata":{"id":"8bzn50l7A2Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model.predict_classes(test_seqs).ravel()\n","\n","print(classification_report(y_test, predictions))\n","pd.DataFrame(confusion_matrix(y_test, predictions))"],"metadata":{"id":"qffdNil8A2Av"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 12: Train Classfier with NNLM Universal Embedding Model\n","\n","__Hint:__ This model should accept the pre-processed text directly (as shown in livecoding)\n"],"metadata":{"id":"uAm5EWSKBCQ1"}},{"cell_type":"code","source":["model = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n","hub_layer = hub.KerasLayer(model, output_shape=[128], input_shape=[], \n","                           dtype=tf.string, trainable=True)"],"metadata":{"id":"65BEiPOhA1-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = tf.keras.models.Sequential()\n","model.add(hub_layer)\n","model.add(tf.keras.layers.Dense(256, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.25))\n","model.add(tf.keras.layers.Dense(256, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.25))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"8_zUsZhLA17a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the model\n","es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                      patience=2,\n","                                      restore_best_weights=True,\n","                                      verbose=1)\n","\n","model.fit(train_clean_text, y_train, \n","          validation_split=0.1,\n","          epochs=EPOCHS, \n","          batch_size=BATCH_SIZE, \n","          shuffle=True,\n","          callbacks=[es],\n","          verbose=1)"],"metadata":{"id":"O1qrOPeNA14t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model.predict_classes(test_clean_text).ravel()\n","\n","print(classification_report(y_test, predictions))\n","pd.DataFrame(confusion_matrix(y_test, predictions))"],"metadata":{"id":"AAnTxqqsBIxI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 13: Train Classfier with BERT\n","\n","##### Note: You might need to restart the notebook environment on colab after installing the below library"],"metadata":{"id":"qGgNiYaNBMEs"}},{"cell_type":"code","source":["!pip install transformers --ignore-installed"],"metadata":{"id":"v2f31tvbBIuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import classification_report, confusion_matrix\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import nltk\n","import matplotlib.pyplot as plt\n","\n","df = pd.read_csv('https://github.com/dipanjanS/text-analytics-with-python/raw/master/media/Womens%20Clothing%20E-Commerce%20Reviews%20-%20NLP.csv', keep_default_na=False)\n","df['Review'] = (df['Title'].map(str) +' '+ df['Review Text']).apply(lambda row: row.strip())\n","df['Recommended'] = df['Recommended IND']\n","df = df[['Review', 'Recommended']]\n","df = df[df['Review'] != '']\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Recommended']), df['Recommended'], test_size=0.3, random_state=42)\n","\n","import nltk\n","import contractions\n","import re\n","import tqdm\n","\n","\n","def normalize_document(doc):\n","    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n","    doc = doc.lower()\n","    doc = contractions.fix(doc)\n","    # lower case and remove special characters\\whitespaces\n","    doc = re.sub(r'[^a-zA-Z\\s]', ' ', doc, re.I|re.A)\n","    doc = re.sub(' +', ' ', doc)\n","    doc = doc.strip()  \n","\n","    return doc\n","\n","def normalize_corpus(docs):\n","    norm_docs = []\n","    for doc in tqdm.tqdm(docs):\n","        norm_doc = normalize_document(doc)\n","        norm_docs.append(norm_doc)\n","\n","    return norm_docs\n","\n","X_train['Clean Review'] = normalize_corpus(X_train['Review'].values)\n","X_test['Clean Review'] = normalize_corpus(X_test['Review'].values)\n","\n","train_clean_text = X_train['Clean Review']\n","test_clean_text = X_test['Clean Review']"],"metadata":{"id":"wTZAMckyBIr8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Train and Evaluate your BERT model using `transformers`"],"metadata":{"id":"g5wu53JTBZG_"}},{"cell_type":"code","source":["import transformers"],"metadata":{"id":"vi5mfYo4BY5r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"q4LZ9LriBIpa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_bert_input_features(tokenizer, docs, max_seq_length):\n","    \n","    all_ids, all_masks, all_segments= [], [], []\n","    for doc in tqdm.tqdm(docs, desc=\"Converting docs to features\"):\n","        \n","        tokens = tokenizer.tokenize(doc)\n","        \n","        if len(tokens) > max_seq_length-2:\n","            tokens = tokens[0 : (max_seq_length-2)]\n","        tokens = ['[CLS]'] + tokens + ['[SEP]']\n","        ids = tokenizer.convert_tokens_to_ids(tokens)\n","        masks = [1] * len(ids)\n","        \n","        # Zero-pad up to the sequence length.\n","        while len(ids) < max_seq_length:\n","            ids.append(0)\n","            masks.append(0)\n","            \n","        segments = [0] * max_seq_length\n","        all_ids.append(ids)\n","        all_masks.append(masks)\n","        all_segments.append(segments)\n","        \n","    encoded = np.array([all_ids, all_masks, all_segments])\n","    \n","    return encoded"],"metadata":{"id":"NYA3SkSVBInK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_SEQ_LENGTH = 121"],"metadata":{"id":"JCcPDzL-BIkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\n","inp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\n","inp_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_segment_ids\")\n","inputs = [inp_id, inp_mask, inp_segment]\n","\n","hidden_state = transformers.TFBertModel.from_pretrained('bert-base-uncased')(inputs)\n","pooled_output = hidden_state[1]\n","dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n","drop1 = tf.keras.layers.Dropout(0.25)(dense1)\n","dense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\n","drop2 = tf.keras.layers.Dropout(0.25)(dense2)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=output)\n","model.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-5, \n","                                           epsilon=1e-08), \n","              loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"id":"1VFe6vVMBIhr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features_ids, train_features_masks, train_features_segments = create_bert_input_features(tokenizer, \n","                                                                                               train_clean_text, \n","                                                                                               max_seq_length=MAX_SEQ_LENGTH)"],"metadata":{"id":"o2u8jg5-BfSk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                      patience=1,\n","                                      restore_best_weights=True,\n","                                      verbose=1)\n","model.fit([train_features_ids, \n","           train_features_masks, \n","           train_features_segments], y_train, \n","          validation_split=0.1,\n","          epochs=3, \n","          batch_size=25, \n","          callbacks=[es],\n","          shuffle=True,\n","          verbose=1)"],"metadata":{"id":"o7gJFJqDBfPl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_features_ids, test_features_masks, test_features_segments = create_bert_input_features(tokenizer, \n","                                                                                            test_clean_text, \n","                                                                                            max_seq_length=MAX_SEQ_LENGTH)\n","print('Test Features:', test_features_ids.shape, test_features_masks.shape, test_features_segments.shape)"],"metadata":{"id":"js9uHQXrBfNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = [1 if pr > 0.5 else 0 \n","                   for pr in model.predict([test_features_ids, \n","                                            test_features_masks, \n","                                            test_features_segments], verbose=0).ravel()]\n","\n","print(classification_report(y_test, predictions))\n","pd.DataFrame(confusion_matrix(y_test, predictions))"],"metadata":{"id":"KUJ6nkimBfKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"sIFaHYGSBfHR"},"execution_count":null,"outputs":[]}]}